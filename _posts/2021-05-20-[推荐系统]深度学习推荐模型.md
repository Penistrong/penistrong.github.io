---
layout:     post
title:      "[推荐系统] 深度学习推荐模型"
subtitle:   "Recommendation model based on Deep Learning"
author:     Penistrong
date:       2021-05-20 20:21:05 +0800
categories: jekyll update
catalog:    true
mathjax:    false
katex:      true
tags:
    - 毕业设计
    - RecSys
    - Deep Learning
---

# 推荐系统

## 深度学习推荐模型

基于深度学习的推荐模型在近5年内已经得到了长足的发展，相对于经典的协同过滤推荐算法而言，深度学习大大加强了模型的拟合能力，增强了推荐模型的推荐效果。
在研究基于深度学习的推荐模型前首先需要梳理主流的深度学习推荐模型的演化图谱，如下图所示。

![主流深度学习推荐模型演化图谱](https://ftp.bmp.ovh/imgs/2021/06/e8c35e64d5ad354e.png)

从上图中可以看到，基于深度学习的推荐模型核心便是深度神经网络中最基础的MLP层，与其他深度学习领域进行交叉后演化出了多种多样的模型结构。Deep Crossing模型是经典的Embedding&MLP架构，主要改进的是MLP应对稀疏特征（Sparse Feature）的能力不足问题。从核心的MLP往下，是Google提出的Wide&Deep模型，它将模型分为Wide和Deep两部分，分别使用深层MLP和单层LR，目的是让模型具备记忆能力和泛化能力。其中，模型的“记忆能力”可以理解为模型直接学习历史数据中用户和物品的共现频率，并将之直接作为推荐依据的能力，通过让模型记住大量的简单直白的重要规则，加强模型的记忆力。而“泛化能力”指的是模型对于新样本和训练中从未出现过的特征组合的预测能力，当出现稀疏新特征时能够作出可靠预测。

随着Wide&Deep的提出，针对其中两部分的不同改进诞生了很多模型。此外，FM（Factorization Machine，因子分解机）也加入了推荐模型的演化图谱中。

本次主要研究Embedding&MLP、NeuralCF、Wide&Deep、DeepFM、DIN这五大模型，以把握深度学习推荐模型的发展脉络。

### Embedding&MLP

深度学习最基础的结构便是多层神经网络MLP（MultiLayer Perceptron，亦称多层感知机），它就像一个黑盒将输入的特征向量进行深度交叉，再输出对目标值的预测。Embedding+MLP的结构是最为经典且应用最广的深度学习推荐模型结构，它在原始特征输入层和MLP之间加入了Embedding层，将输入的稀疏高维特征向量转换为稠密低维Embedding向量，解决了MLP不善于处理稀疏特征的问题。
以微软于2016年提出的Deep Crossing模型为例。该模型是一个经典Embedding&MLP模型架构，结构如下图所示。

![Deep Crossing: 经典的Embedding&MLP架构](https://i.bmp.ovh/imgs/2021/06/119d2db1abc71a62.png)

根据模型结构图显然可以看到，部分原始输入特征层之上多加了一层Embedding层用于将类别型特征对应的稀疏One-hot向量转换成稠密Embedding向量。有些原始特征层并没有加上Embedding层，这些代表数值型特征，直接输入到Stacking层。Stacking层（堆叠层）也称Concatenate层（连接层），它负责将Embedding和数值型特征拼接，形成包含全特征的特征向量再输入上方的MRU（Multiple Residual Units，多层残差网络）层进行训练[28]。MRU层其实就是MLP的一种，除了残差神经元根据具体情况也可选择不同激活函数的其他神经元作为MLP层。MLP层的特点是全连接，如下图所示。

![全连接神经网络示意图](https://i.bmp.ovh/imgs/2021/06/2fc04cecb8e2e784.png)

不同层间的所有神经元两两之间都有连接，其中连接的权重会在训练时梯度反向传播的学习过程中改变。最后便是Scoring层，用于输出预估结果，在CTR预估中采用sigmoid作为该层神经元以解决二分类问题，如果是多分类问题该层一般使用类似softmax的多分类神经元。

### NeuralCF

作为传统推荐算法的协同过滤及其改进的矩阵分解的大致原理是利用用户与物品之间的交互历史行为数据，构建出用户-物品共现矩阵，如下图左所示。

![协同过滤与矩阵分解](https://i.bmp.ovh/imgs/2021/06/62325dd2d7a08e5a.png)

尔后在共现矩阵之上，利用代表用户向量的行向量之间的相似性找到相似用户，再利用相似用户的喜欢物品进行推荐。但是由于共现矩阵往往比较稀疏，如果用户历史行为数据很稀少，找到的相似用户并不足够准确。

于是矩阵分解便被提出以改进协同过滤，它通过将共现矩阵分解为维度更低的用户矩阵和物品矩阵，再从两矩阵中提取用户隐向量（类似Word2vec中的输入向量权重矩阵的行向量作为隐向量），如上图右所示，其实就是类似Embedding方法将稀疏的用户或者物品向量转换为稠密Embedding表达。最后输入层将用户隐向量和物品隐向量的内积作为预测得分，通过跟目标得分对比进行梯度反向传播以更新网络权重。

如果从Embedding&MLP模型结构的角度来看，矩阵分解其实就相当于前者的Embedding层，如下图所示。但是在Embedding层处理之后只是利用内积得到结果，特征尚未充分交叉，显然会导致欠拟合使推荐效果变差。

![矩阵分解的类神经网络模型结构示意图](https://i.bmp.ovh/imgs/2021/06/42d421fec490451b.png)

此即NeuralCF模型的提出目标，它将矩阵分解中的内积操作改变为MLP层，使用户与物品的Embedding表达之间进行充分交叉，以提高模型的拟合能力，如下图所示。

![NeuralCF模型结构示意图](https://i.bmp.ovh/imgs/2021/06/842fc82d9eda2cb4.png)
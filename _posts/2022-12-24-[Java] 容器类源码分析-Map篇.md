---
layout:     post
title:      "[Java] 容器类源码分析-Map篇"
subtitle:   "继承Collection和Map的容器类源码一窥"
author:     Penistrong
date:       2022-12-24 10:49:32 +0800
categories: java
catalog:    true
mathjax:    false
katex:      true
tags:
    - Java
---

# Java容器类源码分析-Map篇

基于**JDK8**的`src.zip`对常用的容器类源码进行简单剖析

## HashMap

HashMap采用了一个`Node`类型(实现了`Map.Entry`接口)的数组作为存放哈希表的基础数据结构，利用位运算(除留余数法的简化版，见[#第三节](#首先计算哈希地址))计算哈希地址，并且使用*拉链法*处理哈希冲突问题

### 存储结构

- 不会被序列化的哈希表数组

  ```java
  transient Node<K,V>[] table;
  ```

- 数组的初始化大小为16，且必须为2的幂，这是为了使用哈希函数时提高效率

  ```java
    /* The default initial capacity - MUST be a power of two. */
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
  ```

- Node是一种单向链表节点结构，同时存储哈希值、键值对和下一个具有同义哈希的节点

  ```java
  static class Node<K,V> implements Map.Entry<K,V> {
      final int hash;
      final K key;
      V value;
      Node<K,V> next;

      Node(int hash, K key, V value, Node<K,V> next) {
          this.hash = hash;
          this.key = key;
          this.value = value;
          this.next = next;
      }

      public final K getKey()        { return key; }
      public final V getValue()      { return value; }
      public final String toString() { return key + "=" + value; }

      public final int hashCode() {
          return Objects.hashCode(key) ^ Objects.hashCode(value);
      }

      public final V setValue(V newValue) {
          V oldValue = value;
          value = newValue;
          return oldValue;
      }

      public final boolean equals(Object o) {
          if (o == this)
              return true;

          return o instanceof Map.Entry<?, ?> e
                  && Objects.equals(key, e.getKey())
                  && Objects.equals(value, e.getValue());
      }
  }
  ```

### 键值对的存储流程

当调用`HashMap::put`时，会使用内建的`HashMap::putVal`函数

```java
public V put(K key, V value) { return putVal(hash(key), key, value, false, true); }

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

#### 首先计算哈希地址

存储键值对时，首先会使用`hash()`函数计算键的哈希值

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

HashMap在存入键为null的K-V对时，会将该K-V对存放在哈希表中索引为0的桶里，否则会将**键的hashCode右移16位后再和其本身做异或**

在`hash()`函数的JavaDoc里可以看到这么做是为了减少哈希冲突，需要结合下面的散列操作一起理解

```java
if ((p = tab[i = (n - 1) & hash]) == null) { ... }
```

一般的哈希函数为除留余数法，除数为哈希表长度。假设当前哈希数组的容量为`n`，它被规定为2的幂次，即 $n=2^p$ (注意n在二进制下一共有`p+1`位)

取余操作如下: 让`hash`去除 $n$，在二进制下商就是`hash`右移 $p$ 位，被右移出去的这 $p$ 个数位组成的就是余数

因为除法操作被编译后其计算速度远不如位运算，为了提高效率HashMap用位运算代替取余操作: `n-1`在二进制下是一串连续的1，可以看作是二进制掩码，将`n-1`直接与`hash`相与，在二进制下相当于提取了`hash`的低 $p$ 位作为余数

虽然位运算取余快捷，但是产生的哈希冲突会比较多，如果直接使用存入键的hashCode作为`hash`去取余，由于采用掩码操作，其高位信息相当于都被丢弃了。比如，当哈希数组初始容量为`n=16`，`n-1`即 $(1111)_b$ 只会保留`hash`的低4位信息

hashCode为32位int类型，可能出现多个键的hashcode低`p`位完全相同而高位大相径庭的情况，但它们都被存在同一个索引的桶中，性能大幅降低

因此在对键初步取哈希值时，会将其hashCode的高位右移16位到int类型的低16位上，再和其本身进行**异或**得到最终的哈希值`hash`，这样高位信息和低位信息获得了混合，就可以减少散列冲突、增大哈希表的散列程度

```java
(h = key.hashCode()) ^ (h >>> 16)
```

#### 接着处理哈希冲突

获得哈希地址即键值对要被存放进的桶下标后

- 如果当前桶为空，则新建1个Node节点放入桶中

- 如果不为空，则在桶中已有节点的单向链表上进行顺序查找，对每个节点的key执行判等操作：使用`==`判断两者地址是否相同或者调用`equals`函数继续判断两个键是否相同

  ```java
  if (p.hash == hash &&
      ((k = p.key) == key || (key != null && key.equals(k))))
  ```

  1. 如果某个已有节点的key与要插入的newKey相同，则将已有节点的value更新为newValue
  2. 否则采用**尾插法**，新建一个节点追加到该桶中链表的尾部

> 从*JDK8*开始，插入桶中链表的操作从*JDK7*的**头插法**改成了尾插法，主要是为了防止对HashMap并发插入时，如果触发了[数组扩容](#扩容原理)，扩容时需要重新分配所有元素的哈希地址，可能会出现同一桶中链表里的两个节点互相指向对方导致死循环

### 键值对的查找流程

1. 计算查找键的哈希地址，时间复杂度为$O(1)$
2. 在对应桶中的单向链表上执行顺序查找，时间复杂度为$O(N)$

假设表长为$M$，键值对总数`size`=$N$，在哈希表比较均匀的情况下，每个桶里链表的平均长度大约是$\frac{N}{M}$，查找的平均复杂度为$O(\frac{N}{M})$

### 扩容原理

HashMap的内置构造函数为

```java
public HashMap(int initialCapacity, float loadFactor) {
    if (initialCapacity < 0)
        throw new IllegalArgumentException("Illegal initial capacity: " +
                                          initialCapacity);
    if (initialCapacity > MAXIMUM_CAPACITY)
        initialCapacity = MAXIMUM_CAPACITY;
    if (loadFactor <= 0 || Float.isNaN(loadFactor))
        throw new IllegalArgumentException("Illegal load factor: " +
                                          loadFactor);
    this.loadFactor = loadFactor;
    this.threshold = tableSizeFor(initialCapacity);
}
```

其他构造函数包括无参和单参的，其中`loadFactor`默认为`0.75f`，初始容量默认为`16`。HashMap允许传入一个不为2的幂次的初始容量，它会调用`tableSizeFor()`函数转换到距离最近的2的幂

| 参数 | 含义 |
| ---- | ---- |
| capacity | 数组容量，为2的幂次，默认为`16` |
| size | 实际存储的键值对数量，也就是对外暴露的`Map::size` |
| threshold | size到达该阈值时就会触发数组扩容，构造HashMap时会通过`tableSizeFor()`计算得到，每次执行`resize()`时以`newThr=(int)newCap * loadFactor`进行更新 |
| loadFactor | 装载因子，即哈希表能够装载的元素比例，默认为`0.75f` |

#### 计算扩容后新数组的容量与阈值

由哈希表的查找流程可知，为了使$O(N/M)$尽可能小，需要哈希表的容量$M$尽可能得大。所以HashMap为了兼顾时间与空间上的效率，采用动态扩容的方式，每当存储的键值对数量`size`超过阈值`threshold`后便执行`resize()`函数

```java
// resize()函数里，新哈希数组的容量及其阈值的计算部分如下
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                  oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;

    ...
}
```

如果哈希数组的旧容量`oldCap`大于等于默认初始容量16，则每次扩容都是将原数组扩大到2倍，相应的，新阈值`newThr`也更新为旧阈值`oldThr`的2倍。在不超出最大容量上限`MaximumCapacity`的其他情况下，新阈值也可以通过`(int)newCap * loadFactor`得到

#### 对旧数组中每个桶中的节点再哈希

扩容后的数组容量和阈值计算完毕后，新建1个新的数组，并遍历旧数组的每个桶，对里面存在的节点执行rehash操作:

```java
// resize()函数里，rehash再哈希的处理部分如下
final Node<K,V>[] resize() {
    ...

    @SuppressWarnings({"rawtypes","unchecked"})
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

1. 如果当前桶里的节点唯一，将节点存入对应的新桶中，地址为`hash & (newCap - 1)`

2. 如果当前桶里的链表存在，无需每次都使用新容量去计算散列地址，因为`newCap`=$2^{p+1}$刚好为`oldCap`=$2^{p}$的2倍，且它们都是2的幂，则`newCap - 1`刚好比`oldCap - 1`多出一个**第`p`位**(最低位从0开始)。对每个节点判断其键*hash*在二进制表示中第`p`位上的值，注意到`oldCap`的二进制表示下**只有第`p`位为1，其他数位都是0**，因此直接使用位运算`e.hash & oldCap`就能够提取第`p`位上的数值：

    - 如果为0, `hash & (newCap - 1) == hash & (oldCap - 1)`，散列地址不变，插入临时低位链表中(`loHead`后面)

    - 如果为1, `hash & (newCap - 1) == hash & (oldCap - 1) + pow(2, p)`，而$2^{p}$的大小就是`oldCap`的值，因此其散列地址就是原散列地址`j`加上`oldCap`，插入临时高位链表中(`hiHead`后面)

    - 最后将低位链表放在散列地址不变的桶中`newTab[j] = loHead`, 高位链表放在原散列地址加上原数组容量的新桶中`newTab[j + oldCap] = hiHead`

    > 再散列的过程中都是采用**尾插**的方式将旧节点插入到新链表的尾部，这样可以保证节点原来的有序性不被破坏

### 桶中链表转换红黑树

HashMap源码中还定义了如下变量

| 参数 | 默认值 | 含义 |
| ---- | ---- | ---- |
| TREEIFY_THRESHOLD | 8 | 桶中节点数超过该阈值后便将链表转换为红黑树(调用`treeifyBin()`) |
| UNTREEIFY_THRESHOLD | 6 | 扩容分桶时使用，原桶里的红黑树结构因为分桶导致节点数减小，小于该阈值后就将红黑树转换为链表(调用`untreeify()`) |
| MIN_TREEIFY_CAPACITY | 64 | `treeifyBin()`中的关键阈值，只有当数组容量`n`超过该阈值后后才会执行转换操作，否则只调用`resize()` |

默认的`TREEIFY_THRESHOLD`也说明里自*JDK8*之后，虽然处理哈希冲突时采用时间复杂度较高的尾插法，但是只要桶中节点数超过`8`且数组总容量超过`64`后就会将桶中链表转换为红黑树，所以尾插法对性能的影响也不算大

> 但是红黑树建树还是比较消耗性能的

## ConcurrentHashMap

- JDK7旧版

  *JDK7*的*ConcurrentHashMap*采用的是分段锁机制，基础数据结构`Segment`继承自可重入锁`ReentrantLock`，每个分段锁都维护着几个桶`HashEntry`，不同线程可以同时访问不同分段锁上的桶，使并发度更高(并发度就是`Segment`的个数)，默认为16

- JDK8新版

  *JDK8*取消了分段锁机制，与*HashMap*一样都是采用Node数组保存数据，但是会利用桶中节点(链表头节点或者红黑树根节点)作为锁对每个桶里的数据加锁，进一步减少并发冲突、提高并发度

对于并发的处理，*JDK8*版本的*ConcurrentMap*引入了CAS机制(Compare And Swap, 比较并交换): 某线程要修改变量$x$的值，首先它会从内存中读取$x$的值设为`a`(称作期待值)，再次从内存中读取$x$的值设为`b`，如果`a == b`，则将$x$更新为目标值，否则不进行修改。CAS的本质是一种基于冲突检测的乐观锁策略: 先进行操作，如果没有其他线程争用共享数据，那么操作就成功了，否则采取补偿措施。CAS是非阻塞同步操作，不像`synchronized`和`ReentrantLock`这两个悲观互斥锁需要通过阻塞其他线程的运行以实现同步

以下都是基于JDK8的ConcurrentHashMap源码进行分析

### 存储结构

与HashMap一样都是使用`Node<K, V>`作为基础结构，维护1个*数组+链表/红黑树*的拉链式哈希表结构，但是在值属性和指向下一个节点的引用属性上都使用了关键字`volatile`作为修饰，目的是在访问节点时实现不用加锁的无锁读

```java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;
    ...
}
```

### get无锁读

JMM(Java Memory Model, Java内存模型)如下图所示，所有变量都存于主存区域中，每个线程都拥有自己的工作内存，其中包含主内存中共享变量的副本。线程进行的所有操作都是在其对应的工作内存中进行，不能直接操作主存也不能操作其他线程的工作内存

![Java内存模型](https://s2.loli.net/2023/03/09/mfsgNK1UEckxnBi.png)

当变量$x$**没有**被`volatile`关键字修饰时，线程操作变量的流程如下：

> 如果线程1要修改$x$的值，它会从主存区将$x$拷贝到自己的工作内存中，然后更新$x$再写回主存中；如果线程2要读取$x$的值，也是去主存区将$x$拷贝进自己的工作内存中再进行读取

这样就会导致可见性问题：比如线程1修改了$x$的值但还没有写回主存，同时线程2要对$x$进行操作，此时线程1所做的修改对线程2不可见。这种因工作内存与主内存存在同步延迟的情况导致了可见性问题

当变量$x$被`volatile`关键字修饰时，相当于标记变量$x$“应当在主存区进行读写操作”，基于内存屏障(Memory Barrier)实现`volatile`变量的内存可见性

对一个`volatile`变量进行写操作时(赋值)，通过jitwatch工具可以查看其编译后得到的汇编代码，在`mov`指令后还有一条`lock cmpxchg`的汇编指令，在多核处理器上执行该指令时会引发两个操作:

1. 将当前核中缓存(Java工作内存)行对应的数据写回到系统内存(Java主存)中
2. 写回系统内存时会使其他核里缓存相同内存地址的数据无效

CPU的不同核之间为了保证缓存一致，实现了缓存一致性协议(MESI)，每个核会嗅探在总线上传播的信号以检查自己缓存的数据是否过期，当它发现某个缓存行对应的内存地址被修改，就会将其设置成无效状态，下次要对这个数据执行任何操作时就会去系统内存中重新将数据读取到缓存里，保证数据是最新的变量副本

volatile关键字保证了多线程下操作的**可见性**，这样在调用`ConcurrentHashMap::get`方法时，可以不用加锁直接进行无锁读，提高并发效率

`ConcurrentHashMap::get`源码如下

```java
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    int h = spread(key.hashCode());
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {
        if ((eh = e.hash) == h) {
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        while ((e = e.next) != null) {
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

根据给定的键key计算Hash以定位对应桶的下标，利用`ConcurrentHashMap::tabAt`封装的volatile读操作，获取对应桶的头节点，如果key不匹配，则继续通过Node的volatile属性next遍历桶里链表中的所有节点，直到key匹配再去读取volatile属性val

以上的3个操作读取的都是volatile修饰的变量，在不加锁的情况下保证了其对应value的内存可见性，读操作也因此线程安全

`ConcurrentHashMap::tabAt`源码如下

```java
// Unsafe mechanics
private static final Unsafe U = Unsafe.getUnsafe();

static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {
    return (Node<K,V>)U.getReferenceAcquire(tab, ((long)i << ASHIFT) + ABASE);
}

/* In jdk.internal.misc.Unsafe */
@IntrinsicCandidate
public final Object getReferenceAcquire(Object o, long offset) {
    return getReferenceVolatile(o, offset);
}

/**
  * Fetches a reference value from a given Java variable, with volatile
  * load semantics. Otherwise identical to {@link #getReference(Object, long)}
  */
@IntrinsicCandidate
public native Object getReferenceVolatile(Object o, long offset);
```

### put流程

put的调用过程大致分为两部分：

- 对非空桶的头节点加上互斥锁`synchronized`，再在其对应的链表上寻找键匹配的节点，对值进行替换

- 对于空桶，采用基于乐观锁策略的CAS机制往桶里添加新的头节点

`put`函数调用的还是内建的`putVal`函数：

```java
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh; K fk; V fv;
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null, new Node<K,V>(hash, key, value)))
                break;                   // no lock when adding to empty bin
        }
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else if (onlyIfAbsent // check first node without acquiring lock
                    && fh == hash
                    && ((fk = f.key) == key || (fk != null && key.equals(fk)))
                    && (fv = f.val) != null)
            return fv;
        else {
            V oldVal = null;
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                    (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key, value);
                                break;
                            }
                        }
                    }
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                        value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                    else if (f instanceof ReservationNode)
                        throw new IllegalStateException("Recursive update");
                }
            }
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}
```

#### put非空桶

找到非空桶后，对其中的头节点加上**互斥锁**，这样其他线程就无法对该桶内的链表进行操作，后续的覆盖或者插入流程与*HashMap*一致

#### put空桶

找到空桶后，调用`casTabAt()`函数，其封装的是*Unsafe*类提供的native方法`compareAndSetReference()`，它与volatile具有相同的内存读写语义

```java
static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i,
                                    Node<K,V> c, Node<K,V> v) {
    return U.compareAndSetReference(tab, ((long)i << ASHIFT) + ABASE, c, v);
}

/* In jdk.internal.misc.Unsafe */
@IntrinsicCandidate
public final native boolean compareAndSetReference(Object o, long offset,
                                                   Object expected,
                                                   Object x);
```

`compareAndSetReference()`函数会比较引用对象o在内存中的地址加上偏移`offset`处的值是否等于期待值`expected`，是的话则将该地址的值替换为给定对象`x`的地址(相当于设置了指向对象`x`的引用)，并返回true

在并发环境下，Put空桶操作基于**自旋锁+CAS**实现：假设有两个线程都在进行put操作，它们插入的键值对相同，且桶内为空：

1. 线程1执行`casTabAt(tab, i, null, node1)`后，期待值`null`与`tab[i]`在内存中的值相等，则将tab[i]所在的内存地址替换为`node1`对象所在的地址，并返回true。新的节点插入成功，此时返回true后执行到`break`一行后结束for死循环`for (Node<K, V>[] tab = table;;)`

2. 线程2在线程1插入键值对之前通过`tabAt`得知相同位置为空桶，但是线程2稍微落后执行`casTabAt(tab, i, null, node2)`，由于期待值`null`与此时`tab[i]`在内存中的实际值不相同(现在`tab[i]`处为`node1`的引用地址)，说明另有线程对`tab[i]`执行了修改，继续执行for死循环，在第二次循环中执行前置的`tabAt()`操作后发现`tab[i]`这个桶不为空，执行[put非空桶](#put非空桶)分支

### 扩容机制



## LinkedHashMap

### 存储结构

*LinkedHashMap*继承自*HashMap*

```java
public class LinkedHashMap<K,V> extends HashMap<K,V> implements Map<K,V> { ... }
```

其存储键值对的基础数据结构`Entry`也是继承自`HashMap::Node`，但是它还包含前一个节点和下一个节点的引用，是一个双向链表的基础节点

```java
static class Entry<K,V> extends HashMap.Node<K,V> {
    Entry<K,V> before, after;
    Entry(int hash, K key, V value, Node<K,V> next) {
        super(hash, key, value, next);
    }
}
```

*LinkedHashMap*内部还维护了另一个双向链表(储存链表的首尾节点)，维护键值对的**插入顺序**或者**LRU顺序**

```java
/* The head (eldest) of the doubly linked list. */
transient LinkedHashMap.Entry<K,V> head;

/* The tail (youngest) of the doubly linked list. */
transient LinkedHashMap.Entry<K,V> tail;
```

LRU(*Least Recently Used*)即最近最少使用算法，它是内存管理的一种经典算法，在*LinkedHashMap*中它主要用来淘汰长时间未被访问的节点

源码中的`accessOrder`变量决定了具体维护的顺序，`true`为LRU顺序，`false`为插入顺序

```java
/** The iteration ordering method for this linked hash map:
  * {@code true} for access-order, {@code false} for insertion-order. */
final boolean accessOrder;
```

每次执行节点访问或插入时，会按照当前设定的顺序`accessOrder`去维护内部双向链表，具体在以下两个函数中实现

```java
void afterNodeInsertion(boolean evict) {}

void afterNodeAccess(Node<K,V> e) {}
```

### LRU顺序链表维护过程

#### afterNodeInsertion

每次执行`put`插入操作后会调用该函数，唯一参数`evict`只有在构造*LinkedHashMap*时为`false`，后面的操作阶段都为`true`，注意*LinkedHashMap*使用的仍是其父类*HashMap*定义的`put`函数，插入后该回调在`HashMap::put`中被调用，见上一大节中的[HashMap::putVal源码](#键值对的存储流程)

当`LinkedHashMap::removeEldestEntry`方法返回`true`时，它会移除内部维护的"**LRU顺序**"双向链表的头节点，即最近最久未被访问的节点

> 但是`removeEldestEntry`方法默认返回`false`，如果想维护LRU顺序，则需要继承*LinkedHashMap*并重写该方法使其返回`true`

```java
void afterNodeInsertion(boolean evict) { // possibly remove eldest
    LinkedHashMap.Entry<K,V> first;
    if (evict && (first = head) != null && removeEldestEntry(first)) {
        K key = first.key;
        removeNode(hash(key), key, null, false, true);
    }
}

protected boolean removeEldestEntry(Map.Entry<K,V> eldest) {
    return false;
}
```

#### afterNodeAccess

当`accessOrder=true`即指定维护LRU顺序时，每当一个节点e被访问后(调用`LinkedHashMap::get`)，会将该节点放到LRU顺序链表的尾部，保证链表尾部是最近访问的节点，这样一来链表头部自然就是最近最久未访问的节点了

```java
public V get(Object key) {
    Node<K,V> e;
    if ((e = getNode(key)) == null)
        return null;
    if (accessOrder)
        afterNodeAccess(e);
    return e.value;
}

void afterNodeAccess(Node<K,V> e) { // move node to last
    LinkedHashMap.Entry<K,V> last;
    if (accessOrder && (last = tail) != e) {
        LinkedHashMap.Entry<K,V> p =
            (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;
        p.after = null;
        if (b == null)
            head = a;
        else
            b.after = a;
        if (a != null)
            a.before = b;
        else
            last = b;
        if (last == null)
            head = p;
        else {
            p.before = last;
            last.after = p;
        }
        tail = p;
        ++modCount;
    }
}
```

### 继承LinkedHashMap实现LRU缓存

根据上一节的分析，如果想使用*LRU缓存*这一数据结构，可以继承*LinkedHashMap*，下面是一个demo

- 设定缓存空间上限为`MAX_ENTRIES=3`
- 调用父类的构造函数时，将`accessOrder`设为`true`
- 重写`removeEldestEntry`方法，使其返回`true`

```java
public class LRUCacheDemo<K, V> extends LinkedHashMap<K, V> {
    private static final int MAX_ENTRIES = 3;

    LRUCacheDemo() {
        super(MAX_ENTRIES, 0.75f, true)
    }

    protected boolean removeEldestEntry(Map.Entry eldest) {
        return true;
    }
}
```

测试一下是否满足LRU顺序，注意默认节点插入顺序跟`HashMap`一样都是**尾插**

```java
public static void main(String[] args) {
    LRUCacheDemo<Integer, String> cache = new LRUCacheDemo<>();

    cache.put(1, "1");  // 插入1: 1
    cache.put(2, "2");  // 插入2: 1->2
    cache.put(3, "3");  // 插入3: 1->2->3
    cache.get(1);       // 访问1，按照LRU算法，节点1被放到当前链表的末尾: 2->3->1
    cache.put(4, "4");  // 插入4，超过了缓存空间上限，移除最近最久未使用: 3->1->4

    System.out.println(cache.keySet()); // 结果为: 3->1->4
}
```

## ThreadLocalMap

ThreadLocalMap是ThreadLocal类内部持有的线程私有变量的哈希表，其中细节很多，见博文[ThreadLocal详解]({})
